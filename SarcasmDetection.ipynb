{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset from a CSV file\n",
        "dataset_file = \"sardet.csv\"\n",
        "df = pd.read_csv(dataset_file)\n",
        "\n",
        "# Display the first few rows of the dataset to get an idea of the data\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # You can perform various text preprocessing steps here, such as removing special characters, URLs, and lowercasing.\n",
        "    # For simplicity, we'll just convert text to lowercase.\n",
        "    return text.lower()\n",
        "\n",
        "df['tweets'] = df['tweets'].apply(preprocess_text)\n",
        "\n",
        "# Display the preprocessed dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTUGR2p4uS0S",
        "outputId": "87937c56-1628-48a5-b85e-def87b272d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              tweets  label\n",
            "0                     I love working midnights tweet      1\n",
            "1  I hate when I buy a bag of air and there's chi...      1\n",
            "2  my grandad always sounds so ill when i speak t...      0\n",
            "3  I realize I'm annoying to everyone, so I won't...      0\n",
            "4  I love when I find these dudes on vine!! #Foll...      1\n",
            "                                              tweets  label\n",
            "0                     i love working midnights tweet      1\n",
            "1  i hate when i buy a bag of air and there's chi...      1\n",
            "2  my grandad always sounds so ill when i speak t...      0\n",
            "3  i realize i'm annoying to everyone, so i won't...      0\n",
            "4  i love when i find these dudes on vine!! #foll...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and obtain embeddings for each tweet\n",
        "def tokenize_and_embed(text):\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Obtain the embeddings from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "\n",
        "    # Get the embeddings for the [CLS] token (or [SEP] if you prefer)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Apply tokenization and embedding to the 'tweets' column in your DataFrame\n",
        "df['embeddings'] = df['tweets'].apply(tokenize_and_embed)\n",
        "\n",
        "# Now, df['embeddings'] contains the BERT embeddings for each tweet\n",
        "# You can convert this to a numpy array for use in machine learning models\n",
        "embeddings = np.vstack(df['embeddings'].to_numpy())\n"
      ],
      "metadata": {
        "id": "k8ntObikueUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Step 4: Feature Engineering - No additional feature engineering is required for this example.\n",
        "\n",
        "# Step 5: Model Selection\n",
        "# Split the data into training and testing sets\n",
        "X = embeddings  # Features\n",
        "y = df['label']  # Labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a machine learning model (Logistic Regression in this example)\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Step 6: Training and Validation\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Hyperparameter Tuning - You can perform hyperparameter tuning here if necessary.\n",
        "\n",
        "# Step 8: Evaluation and Testing\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Print classification report for more detailed evaluation\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Jnd9luhLv1pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef86afcc-9a11-4ce8-9a09-4dcc0ac2614f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.90\n",
            "F1 Score: 0.90\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       203\n",
            "           1       0.90      0.90      0.90       196\n",
            "\n",
            "    accuracy                           0.90       399\n",
            "   macro avg       0.90      0.90      0.90       399\n",
            "weighted avg       0.90      0.90      0.90       399\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming 'model' is your trained machine learning model\n",
        "trained_model = model\n",
        "\n",
        "# Specify the filename for your joblib file\n",
        "joblib_filename = 'sarcasm_detection_model.joblib'\n",
        "\n",
        "# Save the trained model to the joblib file\n",
        "joblib.dump(trained_model, joblib_filename)\n",
        "\n",
        "print(f\"Model saved to {joblib_filename}\")\n"
      ],
      "metadata": {
        "id": "tTiUzIGNv303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa2fe29-420f-4d18-ab04-511fd2176fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to sarcasm_detection_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained LogisticRegression model\n",
        "loaded_model = joblib.load('sarcasm_detection_model.joblib')\n",
        "\n",
        "# Assuming you have the tokenizer and model for BERT as shown in previous steps\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def predict_sarcasm(input_text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Obtain BERT embeddings for the input text\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "\n",
        "    # Get the embeddings for the [CLS] token (or [SEP] if you prefer)\n",
        "    input_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    # Use the loaded LogisticRegression model to make a prediction\n",
        "    prediction = loaded_model.predict([input_embeddings])[0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "prediction = predict_sarcasm(input_text, model, tokenizer)\n",
        "\n",
        "if prediction == 1:\n",
        "    print(\"The text is sarcastic.\")\n",
        "else:\n",
        "    print(\"The text is not sarcastic.\")\n"
      ],
      "metadata": {
        "id": "BhWM-n-iwLTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a7f90d-ee3b-40f1-8e56-24e7d2aa512d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: Oh, the Ultra-Healthy Chocolate-Covered Broccoli – a masterpiece of culinary contradiction! Who needs regular chocolate when you can have this guilt-free delicacy? I mean, who even likes indulgence when you can have the pleasure of eating something green and healthy disguised under a layer of chocolate?  The moment I took a bite of this prodigious creation, my taste buds did a celebratory conga dance. The satisfying crunch of broccoli mingled with the rich, velvety chocolate – what an unmatched symphony of flavors. Forget about those boring old chocolates that don't contribute to your daily dose of vitamins and minerals. This is the epitome of \"having it all.\"\n",
            "The text is sarcastic.\n"
          ]
        }
      ]
    }
  ]
}